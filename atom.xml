<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>鳄鱼守卫</title>
  
  <subtitle>share</subtitle>
  <link href="https://swagger-coder.github.io/atom.xml" rel="self"/>
  
  <link href="https://swagger-coder.github.io/"/>
  <updated>2022-07-18T13:43:44.951Z</updated>
  <id>https://swagger-coder.github.io/</id>
  
  <author>
    <name>Swagger Fei</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>1002 A+B for Polynomials-PAT甲级真题</title>
    <link href="https://swagger-coder.github.io/2022/07/18/1002-A-B-for-Polynomials-PAT%E7%94%B2%E7%BA%A7%E7%9C%9F%E9%A2%98/"/>
    <id>https://swagger-coder.github.io/2022/07/18/1002-A-B-for-Polynomials-PAT%E7%94%B2%E7%BA%A7%E7%9C%9F%E9%A2%98/</id>
    <published>2022-07-18T13:32:17.000Z</published>
    <updated>2022-07-18T13:43:44.951Z</updated>
    
    <content type="html"><![CDATA[<p>This time, you are supposed to find A+B where A and B are two polynomials.</p><h3 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h3><p>Each input file contains one test case. Each case occupies 2 lines, and each line contains the information of a polynomial: K N1 aN1 N2 aN2 … NK aNK, where K is the number of nonzero terms in the polynomial, Ni and aNi (i=1, 2, …, K) are the exponents and coefficients, respectively. It is given that 1 &lt;= K &lt;= 10，0 &lt;= NK &lt; … &lt; N2 &lt; N1 &lt;=1000.</p><h3 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h3><p>For each test case you should output the sum of A and B in one line, with the same format as the input. Notice that there must be NO extra space at the end of each line. Please be accurate to 1 decimal place.</p><h3 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h3><p>2 1 2.4 0 3.2<br>2 2 1.5 1 0.5</p><h3 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h3><p>3 2 1.5 1 2.9 0 3.2</p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>求多项式的和，思路很简单，但是题目有很多细节，比如<code>where K is the number of nonzero terms in the polynomial</code>，这要求不把系数为0的项输出。<code>Notice that there must be NO extra space at the end of each line</code> 这不能只考虑常数项，万一常数项系数为0就不行了。<br><br>这题一开始想到了字典，但实现过程中发现并不方便，<code>c++</code>字典会按<code>key</code>从小到大排序，但依次访问只能通过迭代器(<code>begin()</code>、<code>end()</code>)，并且初始化值得依靠<code>find()</code>查找<code>key</code>是否存在。这里看到了<a href="https://www.liuchuo.net/archives/1890">柳诺的实现</a>，太简洁了，所以以 <code>int</code>为<code>key</code>的字典，大可以考虑用数组实现。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    map&lt;<span class="keyword">int</span>, <span class="keyword">double</span>&gt; m;            <span class="comment">// key：指数，value：系数</span></span><br><span class="line">    <span class="keyword">int</span> k, n;                      <span class="comment">// k：非零项数量，n：指数</span></span><br><span class="line">    <span class="keyword">double</span> a;                      <span class="comment">// a：系数</span></span><br><span class="line">    map&lt;<span class="keyword">int</span>, <span class="keyword">double</span>&gt;::iterator it; <span class="comment">// 迭代器</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;k);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; k; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d %lf&quot;</span>, &amp;n, &amp;a);</span><br><span class="line">            <span class="comment">// 查找字典，无n赋值，有n累加</span></span><br><span class="line">            it = m.<span class="built_in">find</span>(n);</span><br><span class="line">            <span class="keyword">if</span> (it == m.<span class="built_in">end</span>()) <span class="comment">// find()返回迭代器，没找到指向end()</span></span><br><span class="line">                m[n] = a;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                m[n] += a;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>; <span class="comment">// 只考虑非零项！！！</span></span><br><span class="line">    <span class="keyword">for</span> (it = m.<span class="built_in">begin</span>(); it != m.<span class="built_in">end</span>(); it++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (it-&gt;second != <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d&quot;</span>, count);</span><br><span class="line">    <span class="comment">// map按key自动从小到大排序</span></span><br><span class="line">    <span class="keyword">for</span> (it = m.<span class="built_in">end</span>(); it != m.<span class="built_in">begin</span>(); it--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (it == m.<span class="built_in">end</span>())</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">if</span> (it-&gt;second == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot; %d %.1f&quot;</span>, it-&gt;first, it-&gt;second);</span><br><span class="line">    &#125;</span><br><span class="line">    it = m.<span class="built_in">begin</span>();</span><br><span class="line">    <span class="keyword">if</span> (it-&gt;second != <span class="number">0</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot; %d %.1f&quot;</span>, it-&gt;first, it-&gt;second);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;This time, you are supposed to find A+B where A and B are two polynomials.&lt;/p&gt;
&lt;h3 id=&quot;Input&quot;&gt;&lt;a href=&quot;#Input&quot; class=&quot;headerlink&quot; title=&quot;</summary>
      
    
    
    
    <category term="PAT甲级真题" scheme="https://swagger-coder.github.io/categories/PAT%E7%94%B2%E7%BA%A7%E7%9C%9F%E9%A2%98/"/>
    
    <category term="字典" scheme="https://swagger-coder.github.io/categories/PAT%E7%94%B2%E7%BA%A7%E7%9C%9F%E9%A2%98/%E5%AD%97%E5%85%B8/"/>
    
    
  </entry>
  
  <entry>
    <title>链表(c++版)</title>
    <link href="https://swagger-coder.github.io/2022/07/17/%E9%93%BE%E8%A1%A8-c-%E7%89%88/"/>
    <id>https://swagger-coder.github.io/2022/07/17/%E9%93%BE%E8%A1%A8-c-%E7%89%88/</id>
    <published>2022-07-17T05:51:05.000Z</published>
    <updated>2022-07-18T14:08:40.822Z</updated>
    
    
    
    
    <category term="算法" scheme="https://swagger-coder.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>上下班读书系列：《黄金时代》</title>
    <link href="https://swagger-coder.github.io/2022/07/13/%E4%B8%8A%E4%B8%8B%E7%8F%AD%E8%AF%BB%E4%B9%A6%E7%B3%BB%E5%88%97%EF%BC%9A%E3%80%8A%E9%BB%84%E9%87%91%E6%97%B6%E4%BB%A3%E3%80%8B/"/>
    <id>https://swagger-coder.github.io/2022/07/13/%E4%B8%8A%E4%B8%8B%E7%8F%AD%E8%AF%BB%E4%B9%A6%E7%B3%BB%E5%88%97%EF%BC%9A%E3%80%8A%E9%BB%84%E9%87%91%E6%97%B6%E4%BB%A3%E3%80%8B/</id>
    <published>2022-07-13T01:06:51.000Z</published>
    <updated>2022-07-13T09:32:42.280Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="时代背景"><a href="#时代背景" class="headerlink" title="时代背景"></a>时代背景</h3><h3 id="时间表"><a href="#时间表" class="headerlink" title="时间表"></a>时间表</h3><h2 id="名句摘录"><a href="#名句摘录" class="headerlink" title="名句摘录"></a>名句摘录</h2><h2 id="感想-amp-思考"><a href="#感想-amp-思考" class="headerlink" title="感想&amp;思考"></a>感想&amp;思考</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;h3 id=&quot;时代背景&quot;&gt;&lt;a href=&quot;#时代背景&quot; class=&quot;headerlink&quot; title=&quot;时代背景&quot;&gt;&lt;/a&gt;时代背景&lt;/h</summary>
      
    
    
    
    <category term="读后感" scheme="https://swagger-coder.github.io/categories/%E8%AF%BB%E5%90%8E%E6%84%9F/"/>
    
    
  </entry>
  
  <entry>
    <title>问题 A: Set Similarity (25)</title>
    <link href="https://swagger-coder.github.io/2022/07/10/%E9%97%AE%E9%A2%98-A-Set-Similarity-25/"/>
    <id>https://swagger-coder.github.io/2022/07/10/%E9%97%AE%E9%A2%98-A-Set-Similarity-25/</id>
    <published>2022-07-10T10:33:07.000Z</published>
    <updated>2022-07-10T10:46:19.838Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p>Given two sets of integers, the similarity of the sets is defined to be Nc/Nt*100%, where Nc is the number of distinct common numbers shared by the two sets, and Nt is the total number of distinct numbers in the two sets. Your job is to calculate the similarity of any given pair of sets.<br></p><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><p>Each input file contains one test case. Each case first gives a positive integer N (&lt;=50) which is the total number of sets. Then N lines follow, each gives a set with a positive M (&lt;=104) and followed by M integers in the range [0, 109]. After the input of sets, a positive integer K (&lt;=2000) is given, followed by K lines of queries. Each query gives a pair of set numbers (the sets are numbered from 1 to N). All the numbers in a line are separated by a space.<br></p><h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><p>For each query, print in one line the similarity of the sets, in the percentage form accurate up to 1 decimal place.<br></p><h3 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h3><p>3<br><br>3 99 87 101<br><br>4 87 101 5 87<br><br>7 99 101 18 5 135 18 99<br><br>2<br><br>1 2<br><br>1 3<br></p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>这道题是在熟悉<code>set</code>，熟悉<code>set</code>操作即可。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 头文件</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;set&gt;</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义</span></span><br><span class="line">set&lt;<span class="keyword">int</span>&gt; st;</span><br><span class="line">set&lt;<span class="keyword">int</span>&gt;::iterator it = st.<span class="built_in">begin</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 访问</span></span><br><span class="line">*it <span class="comment">// set只能通过迭代器访问元素（仅vector和string支持*（it+i））</span></span><br><span class="line"><span class="keyword">for</span>(it = st.<span class="built_in">begin</span>(); it != st.<span class="built_in">end</span>(); it++)&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;%d&quot;</span>, *it);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 插入</span></span><br><span class="line">st.<span class="built_in">insert</span>(<span class="number">1</span>) <span class="comment">//插入，是在st上，不是在迭代器</span></span><br><span class="line"><span class="comment">// 查找</span></span><br><span class="line">set&lt;<span class="keyword">int</span>&gt;::iterator it = st.<span class="built_in">find</span>(<span class="number">2</span>);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, *it); <span class="comment">//在set中查找2，返回其迭代器</span></span><br><span class="line"><span class="comment">// 删除</span></span><br><span class="line">st.<span class="built_in">erase</span>(it); <span class="comment">// it为所需要删除元素的迭代器</span></span><br><span class="line">st.<span class="built_in">erase</span>(<span class="number">2</span>); <span class="comment">// 直接传值删除</span></span><br><span class="line"><span class="comment">// 大小</span></span><br><span class="line">st.<span class="built_in">size</span>();</span><br><span class="line"><span class="comment">// 清空</span></span><br><span class="line">st.<span class="built_in">clear</span>();</span><br></pre></td></tr></table></figure><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;set&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n, m, k, temp, a, b;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;n);</span><br><span class="line">    vector&lt;set&lt;<span class="keyword">int</span>&gt;&gt; <span class="built_in">v</span>(n);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;m);</span><br><span class="line">        set&lt;<span class="keyword">int</span>&gt; s;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; m; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;temp);</span><br><span class="line">            <span class="comment">// set插入元素</span></span><br><span class="line">            s.<span class="built_in">insert</span>(temp);</span><br><span class="line">        &#125;</span><br><span class="line">        v[i] = s;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;k);</span><br><span class="line">    <span class="comment">// set迭代器</span></span><br><span class="line">    set&lt;<span class="keyword">int</span>&gt;::iterator it;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d %d&quot;</span>, &amp;a, &amp;b);</span><br><span class="line">        <span class="comment">// 求集合并集和交集的操作</span></span><br><span class="line">        <span class="keyword">int</span> nc = <span class="number">0</span>, nt = v[b - <span class="number">1</span>].<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">for</span> (it = v[a - <span class="number">1</span>].<span class="built_in">begin</span>(); it != v[a - <span class="number">1</span>].<span class="built_in">end</span>(); it++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// find(*it)查找元素，没找到指向末尾旁边；同时end()也指向末尾旁边</span></span><br><span class="line">            <span class="comment">// ps:*(st.end())会返回最后一个元素</span></span><br><span class="line">            <span class="keyword">if</span> (v[b - <span class="number">1</span>].<span class="built_in">find</span>(*it) == v[b - <span class="number">1</span>].<span class="built_in">end</span>())</span><br><span class="line">                nt++;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                nc++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// double注意类型转换</span></span><br><span class="line">        <span class="keyword">double</span> ans = (<span class="keyword">double</span>)nc / nt * <span class="number">100</span>;</span><br><span class="line">        <span class="comment">// .1f 保留一位小数</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%.1f%%\n&quot;</span>, ans);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;h3 id=&quot;描述&quot;&gt;&lt;a href=&quot;#描述&quot; class=&quot;headerlink&quot; title=&quot;描述&quot;&gt;&lt;/a&gt;描述&lt;/h3&gt;&lt;p&gt;Giv</summary>
      
    
    
    
    <category term="算法 - Set" scheme="https://swagger-coder.github.io/categories/%E7%AE%97%E6%B3%95-Set/"/>
    
    
    <category term="算法" scheme="https://swagger-coder.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>软件杯-智能创作平台赛题总结</title>
    <link href="https://swagger-coder.github.io/2022/07/09/%E8%BD%AF%E4%BB%B6%E6%9D%AF-%E6%99%BA%E8%83%BD%E5%88%9B%E4%BD%9C%E5%B9%B3%E5%8F%B0%E8%B5%9B%E9%A2%98%E6%80%BB%E7%BB%93/"/>
    <id>https://swagger-coder.github.io/2022/07/09/%E8%BD%AF%E4%BB%B6%E6%9D%AF-%E6%99%BA%E8%83%BD%E5%88%9B%E4%BD%9C%E5%B9%B3%E5%8F%B0%E8%B5%9B%E9%A2%98%E6%80%BB%E7%BB%93/</id>
    <published>2022-07-09T03:39:42.000Z</published>
    <updated>2022-07-10T10:21:03.693Z</updated>
    
    <content type="html"><![CDATA[<h2 id="赛题简介"><a href="#赛题简介" class="headerlink" title="赛题简介"></a>赛题简介</h2><p><a href="http://www.cnsoftbei.com/plus/view.php?aid=729">A9-智能创作平台</a><br><br>根据赛题，我在进行需求分析后得到以下几个功能点</p><ul><li>单文本处理<ul><li>文本输入</li><li>标题生成</li><li>摘要生成</li><li>结果显示</li></ul></li><li>多文本批量处理<ul><li>文件上传</li><li>标题生成</li><li>摘要生成</li><li>文件下载</li></ul></li><li>系统参数配置<ul><li>中英文切换</li><li>生成参数配置</li></ul></li></ul><p>我的预期是想做成<code>ilovePDF</code>类似的工具型网站，于是索性不做登陆注册了。因此这个网站的架构就非常简单了，不需要用到数据库，当然如果要做历史记录查看还是需要的。</p><h2 id="技术路线"><a href="#技术路线" class="headerlink" title="技术路线"></a>技术路线</h2><p><img src="/2022/07/09/%E8%BD%AF%E4%BB%B6%E6%9D%AF-%E6%99%BA%E8%83%BD%E5%88%9B%E4%BD%9C%E5%B9%B3%E5%8F%B0%E8%B5%9B%E9%A2%98%E6%80%BB%E7%BB%93/1.png"><br>项目最后是成功完成了，但作为cv工程师，我只是代码的搬运工罢了。</p><ul><li>streamlit开发文档<a href="https://docs.streamlit.io/">Docs</a></li><li>开发的整体思路借鉴<a href="https://link.zhihu.com/?target=https://github.com/liucongg/GPT2-NewsTitle">超详细中文注释的GPT2新闻标题生成项目</a></li><li>文件上传下载功能借鉴<a href="https://github.com/streamlit/example-app-csv-wrangler">✂️ CSV Wrangler</a></li><li>摘要抽取算法<a href="https://github.com/ArtistScript/FastTextRank">FastTextRank</a>或<a href="https://github.com/letiantian/TextRank4ZH">TextRank4ZH</a></li><li>文本生成模型<a href="https://github.com/ZhuiyiTechnology/WoBERT">WoBERT</a>（rouge评测结果很好）</li></ul><h2 id="反思总结"><a href="#反思总结" class="headerlink" title="反思总结"></a>反思总结</h2><ul><li><p>技术路线选择</p><ul><li>由于自己没有人工智能基础，导致文本生成模型一开始只能依赖现成的项目，结果就是<code>rouge</code>评测效果很差</li><li>知道模型不行之后还想着从头学，导致自己渐渐被技术吓到，其实正确思路还是找现成的模型，能尽快使用</li><li>教训就是在做之前需要对现有的模型算法有整体的了解，并一开就就要做好多方面比较的准备，选取最优方案</li></ul></li><li><p>环境配置</p><ul><li><p>环境配置和开发的时间感觉都快五五分了，通过本次比赛自己对环境方面也算是踩坑有得了</p></li><li><p>首先是<code>pytorch</code>、<code>tensorflow</code>使用<code>win</code>下<code>gpu</code>，不同版本的需对应不同<code>cuda</code>版本</p><ul><li><code>torch 1.10.1 + transfoemers 3.0.2 + cuda 11.3</code></li><li><code>bert4keras 0.8.8 + tensorflow 2.1.0 + Keras 2.3.1 + cuda 10.1</code></li></ul></li><li><p>虚拟环境也需要注意了，使用<code>pycharm</code>管理一定要把虚拟环境名改成不一样的。这里我已经不推荐使用<code>pycharm</code>进行管理了，还是直接用<code>virtualenv</code>或<code>conda</code>比较靠谱</p></li><li><p>被<code>win</code>坑的，早日转<code>Linux</code></p></li></ul></li><li><p>复用</p><ul><li>这次项目里的<code>rouge</code>评测之后可以一直用</li><li><code>streamlit</code>方面的组件，如<code>.csv</code>文件的上传下载</li></ul></li><li><p>后续任务</p><ul><li>看下文本生成方面的综述，研究下主流的方法，并自己实验比较</li><li>自己搭建一个文本生成的模型，考虑用<code>keras</code>，从代码量来说，这个真的很香</li><li>网站架构方面，如果用<code>Java</code>框架，将怎么与<code>python</code>的代码融合？是时候进修下<code>Java web</code>了。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;赛题简介&quot;&gt;&lt;a href=&quot;#赛题简介&quot; class=&quot;headerlink&quot; title=&quot;赛题简介&quot;&gt;&lt;/a&gt;赛题简介&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://www.cnsoftbei.com/plus/view.php?aid=729&quot;&gt;A9-智能</summary>
      
    
    
    
    <category term="总结" scheme="https://swagger-coder.github.io/categories/%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="项目总结" scheme="https://swagger-coder.github.io/tags/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/"/>
    
    <category term="streamlit" scheme="https://swagger-coder.github.io/tags/streamlit/"/>
    
    <category term="文本生成" scheme="https://swagger-coder.github.io/tags/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>what is lean manufacturing?</title>
    <link href="https://swagger-coder.github.io/2022/07/04/what-is-lean-manufacturing/"/>
    <id>https://swagger-coder.github.io/2022/07/04/what-is-lean-manufacturing/</id>
    <published>2022-07-04T08:11:03.000Z</published>
    <updated>2022-07-10T10:11:47.208Z</updated>
    
    <content type="html"><![CDATA[<p>KIT第一天学习任务，了解什么是精益生产。概念上理解很简单，一言以蔽之，精益生产就是以用户价值为核心，减少浪费，提高收益。但其中有很多原则和概念需要记忆，因此简单整理笔记加深自己记忆，更多內容可以查看<br><a href="https://www.leanproduction.com/essence-of-lean/">Essence of Lean – Eliminating Waste (Muda) | Lean Production</a>。</p><p><img src="/2022/07/04/what-is-lean-manufacturing/lean.png"></p><h2 id="definition"><a href="#definition" class="headerlink" title="definition"></a>definition</h2><ul><li>a methodology(方法论) that focuses on minimizing waste within manufacturing system while simultaneously(同时) maximizing productivity.</li><li>Lean manufacturing is a production method aimed primarily at reducing times within the production system as well as response times from suppliers and to customers.</li><li>The core idea of lean manufacturing is quite simple: relentlessly work on eliminating waste from the manufacturing process.</li></ul><h2 id="five-lean-principles"><a href="#five-lean-principles" class="headerlink" title="five lean principles"></a>five lean principles</h2><ul><li><p><strong>Identify and Define Value：</strong> Look at your process from the perspective of your <strong>customers</strong> and identify what they deem valuable(从客户视角审视生产过程，并搞清楚他们严重的价值所在)<br><strong>how to ?</strong> just identify</p><ul><li>what customer really want</li><li>what is customer willing to pay for</li><li>what does customer perceive as valuable</li></ul></li><li><p><strong>Map the Value Stream:</strong> Distinguish steps and activities in the process that provide value from those that create waste</p><p>how to?<br>ask three questions: </p><ul><li>Does this step, and its activities, bring value to the customer? </li><li>move your product or service downstream towards the customer</li><li>flow smoothly, without impediments(阻碍), within the context of the value stream?</li></ul><p>and a step can be categorized into <strong>three levels</strong>: add value, create necessary waste, create unnecessary waste</p></li><li><p><strong>Create Smooth Flow:</strong> Streamline(精简) the process to be free of bottlenecks and delays</p><p>how to?</p><ul><li>reduce changover times</li><li>avoid batch operations(使流程清晰)</li><li>organize equipment(工具与流程相符)</li></ul></li><li><p><strong>Pull based on customer demand:</strong> Align to customer <strong>demand</strong> and adjust quickly to changes in that demand（抓住客户需求，快速迭代需求）</p></li><li><p><strong>Strive for Perfection:</strong> Promote an internal culture of continuous improvement and excellence(精益求精，形成公司文化) </p></li></ul><p>The five lean principles are an iterative process that will bring further value as they are repeated</p><h2 id="seven-delay-wastes"><a href="#seven-delay-wastes" class="headerlink" title="seven delay wastes"></a><strong>seven delay wastes</strong></h2><ul><li><p>overprocduction（超产）</p><ul><li>making something before it is truly needed</li></ul></li><li><p>waiting</p></li><li><p>transport</p><ul><li>物资的无意义移动</li></ul></li><li><p>motion</p><ul><li>人员的无意义移动</li></ul></li><li><p>inventory（存货）</p><ul><li><p>Product (raw materials, work-in-process, or finished goods) quantities that go beyond supporting the immediate need.</p></li><li><p>vs overprocduction</p></li></ul></li><li><p>overprocessing</p></li><li><p>defects</p></li></ul><h2 id="an-eight-delay-waste"><a href="#an-eight-delay-waste" class="headerlink" title="an eight delay waste"></a><strong>an eight delay waste</strong></h2><ul><li>unused human potential</li><li>reasons<ul><li>responsibility lies squrely(直接) on the shoulders of management</li><li>management policies and management styles that diminish(减少) employee contributions</li></ul></li><li>countermeasure<ul><li>developing strong coaching skills for managers</li></ul></li></ul><h2 id="25-tools"><a href="#25-tools" class="headerlink" title="25 tools"></a>25 tools</h2><p><a href="https://www.leanproduction.com/top-25-lean-tools/">top-25-lean-tools</a></p><ul><li>5s</li><li>andon</li><li><strong>Jidoka</strong>(自动化)</li><li>Just-In-Time (JIT)</li><li>PDCA(Plan, Do, Check, Act)<br><br>……</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;KIT第一天学习任务，了解什么是精益生产。概念上理解很简单，一言以蔽之，精益生产就是以用户价值为核心，减少浪费，提高收益。但其中有很多原则和概念需要记忆，因此简单整理笔记加深自己记忆，更多內容可以查看&lt;br&gt;&lt;a href=&quot;https://www.leanproducti</summary>
      
    
    
    
    
    <category term="lean" scheme="https://swagger-coder.github.io/tags/lean/"/>
    
    <category term="KIT" scheme="https://swagger-coder.github.io/tags/KIT/"/>
    
  </entry>
  
  <entry>
    <title>vue-element-admin+django 前后端交互</title>
    <link href="https://swagger-coder.github.io/2022/06/20/vue-element-admin-django-%E5%89%8D%E5%90%8E%E7%AB%AF%E4%BA%A4%E4%BA%92/"/>
    <id>https://swagger-coder.github.io/2022/06/20/vue-element-admin-django-%E5%89%8D%E5%90%8E%E7%AB%AF%E4%BA%A4%E4%BA%92/</id>
    <published>2022-06-20T13:54:12.000Z</published>
    <updated>2022-07-10T10:11:45.252Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Linux期末复习</title>
    <link href="https://swagger-coder.github.io/2022/06/15/Linux%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/"/>
    <id>https://swagger-coder.github.io/2022/06/15/Linux%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/</id>
    <published>2022-06-15T06:31:04.000Z</published>
    <updated>2022-06-15T13:14:47.224Z</updated>
    
    <content type="html"><![CDATA[<p>期末复习按照提问式覆盖知识点，考点，希望一下午能把除编程以外的基础知识整理完</p><h2 id="Linux概述"><a href="#Linux概述" class="headerlink" title="Linux概述"></a>Linux概述</h2><ul><li><p>简述Linux的历史（Linux常见术语）</p><ul><li>UNIX操作系统，Linux的前身，Linux本身就是一种类UNIX操作系统。</li><li>1984，Minix诞生，后来林纳斯在Minix的基础上开发了Linux。</li><li>1984，GNU计划，GNU也是一个类Unix操作系统。它是由多个应用程序、系统库、开发工具乃至游戏构成的程序集合。（为linux提供应用程序）</li><li>GPL（GNU Public Lincense）GNU通用公共许可证。开源协议。</li><li>POSIX，可移植操作系统接口。接口标准。</li><li>总结：Linux是在UNIX和Minix基础上，集成了GNU计划软件，符合GPL开源协议，符合POSIX接口标准的这么一个操作系统。</li></ul></li><li><p>Linux操作系统的版本</p><ul><li>内核，通过<code>uname -a</code>查看</li><li>发行版（Ubuntu、Debian、CentOS、Red Hat）</li></ul></li><li><p>Linux操作系统的主要应用领域</p><ul><li>传统企业级服务器应用场景</li><li>嵌入式、物联网、边缘计算等应用场景</li><li>智能手机、平板电脑等移动终端</li><li>云计算、区块链、大数据、深度学习等应用场</li><li>个人桌面操作系统</li></ul></li></ul><h2 id="Linux命令行基础"><a href="#Linux命令行基础" class="headerlink" title="Linux命令行基础"></a>Linux命令行基础</h2><ul><li><p>将test文件夹下所有文件赋值到test1</p><ul><li><code>cp -i test/* test1</code></li></ul></li><li><p>(输出重定向)把test.txt的文件內容加上行号后追加到test1.txt文件，多行换成一行</p><ul><li><code>cat -ns test.txt &gt;&gt; test1.txt </code></li></ul></li><li><p>(管道)从<code>etc/passwd</code>文件中查找zp记录</p><ul><li><code>cat /etc/passwd | grep zp</code></li></ul></li><li><p>输出文件行号</p><ul><li><code>cat -n file</code></li><li><code>nl -b a file</code></li><li>进阶<code>nl -b a -n rz -w 3 xattr.conf</code>：不论是否空行都列出行号，行号在栏位的最右方显示，且加0，占3位</li></ul></li><li><p>打印文件的开头十行</p><ul><li><code>head -n 10 file</code></li><li>(打印文件的最后十行)<code>tail in 10 file</code></li></ul></li></ul><h2 id="文件与目录管理"><a href="#文件与目录管理" class="headerlink" title="文件与目录管理"></a>文件与目录管理</h2><ol><li>什么是软链接与硬连接？有什么区别？<br><br>① 在Linux系统中，内核会为每个新创建的文件分配一个Inode（索引节点），每个文件有唯一的inode号。文件属性保存在索引结点里，在访问文件时，索引结点被复制到内存在，从而实现文件的快速访问。<br><br>② 链接是一种方便用户快速访问的方式。Linux中包括两种链接：硬链接(Hard Link)和软链接(Soft Link),软链接又称为符号链接（Symbolic link）。符号连接相当于Windows下的快捷方式。<br><br>③ 硬链接其实就是指针，指向文件的inode，只有当文件的硬连接数为0时，才能将文件真正从磁盘上删除。不足之处a.在于不可以在不同文件系统的文件间建立链接 b.只有超级用户才可以为目录创建硬链接。 命令：<code>ln file file_link</code>。<br><br>④ 软链接实际是建立一个新文件，文件大小与原文件不同；软连接可以对一个不存在的文件名进行连接；软连接可以对目录进行连接。<br><br>补：<br><br>（1）软链接文件<br><br>• 软链接文件又叫符号链接文件，这个文件包含了另一个文件的路径名。其可以是任意文件或目录，可以链接不同文件系统的文件。在对软链接文件进行读写的时候，系统会自动地把该操作转换为对源文件的操作，但删除软链接文件时，系统仅仅删除软链接文件，而不删除源文件本身。<br><br>• 用“ls -l”命令查看某个文件的属性，可以看到有类似“lrwxrwxrwx”的属性符号，其属性第一个符号是“l”，这样的文件在Linux系统中就是软链接文件。<br><br>（2）硬链接文件<br><br>• 硬链接是已存在文件的另一个文件，对硬链接文件进行读写和删除操作时，结果和软链接相同。但如果删除硬链接文件的源文件，硬链接文件仍然存在，而且保留了原有的内容。这时，系统就“忘记”了它曾经是硬链接文件，而把它当成一个普通文件。<br><br>• 用“ls -l”命令查看某个文件的属性，可以看到第二列的文件硬链接数大于1 ，这样的文件在Linux系统中就是硬链接文件。<br></li></ol><p>详细看<a href="https://www.cnblogs.com/xiaochaohuashengmi/archive/2011/10/05/2199534.html">linux下创建和删除软、硬链接</a></p><ol start="2"><li>常见的Linux文件类型有哪些？<br><br>占用存储空间的类型：文件、目录、符号链接。符号链接记录的是路径，路径不长时存在innode里面。其他四种：套接字、块设备、字符设备、管道是伪文件，不占用磁盘空间。</li></ol><table><thead><tr><th>文件类型标识</th><th>文件类型</th></tr></thead><tbody><tr><td>-</td><td>普通文件</td></tr><tr><td>d</td><td>目录</td></tr><tr><td>l</td><td>符号链接</td></tr><tr><td>s（伪文件）</td><td>套接字</td></tr><tr><td>b（伪文件）</td><td>块设备</td></tr><tr><td>c（伪文件）</td><td>字符设备</td></tr><tr><td>p（伪文件）</td><td>管道</td></tr></tbody></table><p>详细见<a href="https://blog.csdn.net/m0_37302219/article/details/110821491">Linux期末复习-Linux简答题1-4章</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;期末复习按照提问式覆盖知识点，考点，希望一下午能把除编程以外的基础知识整理完&lt;/p&gt;
&lt;h2 id=&quot;Linux概述&quot;&gt;&lt;a href=&quot;#Linux概述&quot; class=&quot;headerlink&quot; title=&quot;Linux概述&quot;&gt;&lt;/a&gt;Linux概述&lt;/h2&gt;&lt;ul&gt;
&lt;l</summary>
      
    
    
    
    
    <category term="Linux" scheme="https://swagger-coder.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu重装指南</title>
    <link href="https://swagger-coder.github.io/2022/06/13/Ubuntu%E9%87%8D%E8%A3%85%E6%8C%87%E5%8D%97/"/>
    <id>https://swagger-coder.github.io/2022/06/13/Ubuntu%E9%87%8D%E8%A3%85%E6%8C%87%E5%8D%97/</id>
    <published>2022-06-13T10:42:52.000Z</published>
    <updated>2022-07-10T10:11:53.851Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>网上教程很多，这里添几个，以备不时之需</p><ul><li><a href="https://zhuanlan.zhihu.com/p/141033713">ubuntu安装</a></li><li><a href="https://blog.csdn.net/xiangxianghehe/article/details/105688062">apt源配置</a></li><li><a href="https://zhuanlan.zhihu.com/p/28544384">xshell连接</a></li></ul><h2 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h2><ul><li>查看版本信息: <code>cat /proc/version</code>、<code>uname -a</code></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h2&gt;&lt;p&gt;网上教程很多，这里添几个，以备不时之需&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>tensorboardX 安装踩坑</title>
    <link href="https://swagger-coder.github.io/2022/06/04/tensorboardX-%E5%AE%89%E8%A3%85%E8%B8%A9%E5%9D%91/"/>
    <id>https://swagger-coder.github.io/2022/06/04/tensorboardX-%E5%AE%89%E8%A3%85%E8%B8%A9%E5%9D%91/</id>
    <published>2022-06-04T08:43:33.000Z</published>
    <updated>2022-07-10T10:11:59.799Z</updated>
    
    <content type="html"><![CDATA[<h2 id="官网看环境"><a href="#官网看环境" class="headerlink" title="官网看环境"></a>官网看环境</h2><p><a href="https://github.com/lanpa/tensorboardX">tensorboardX-github</a><br>写在前面的话也很重要，ta给出了前置条件，首先要安装好<code>pytorch</code>、<code>torchvision</code>、<code>tensorboard</code>。<br><br>在开始安装前，我的环境如下</p><ul><li>python 3.7</li><li>torch 1.10.1+cu113</li><li>torchvion 1.10.1+cu113</li></ul><p>所以在安装<code>tensorboardX</code>之前，首先要安装<code>tensorboard</code></p><h2 id="tensorboard安装"><a href="#tensorboard安装" class="headerlink" title="tensorboard安装"></a>tensorboard安装</h2><ol><li>tensorflow安装<br><br>安装<code>tensorboard</code>还需要先安装<code>tensorflow</code>（套娃属于是），tensorflow切记安装最新版本，若不是最新执行以下命令<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade tensorflow -i http://pypi.douban.com/</span><br><span class="line">simple --trusted-host pypi.douban.com</span><br></pre></td></tr></table></figure></li></ol><ul><li>tensorflow 2.9.1</li></ul><ol start="2"><li>tensorboard安装<br>直接pip安装即可<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorboard -i http://pypi.douban.com/</span><br><span class="line">simple --trusted-host pypi.douban.com</span><br></pre></td></tr></table></figure></li></ol><ul><li>tensorboard 2.9.0 </li></ul><h2 id="正式安装"><a href="#正式安装" class="headerlink" title="正式安装"></a>正式安装</h2><ol><li>依赖环境安装<br><br>官网上只说需要<code>sondfile</code>依赖<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install soundfile -i http://pypi.douban.com/</span><br><span class="line">simple --trusted-host pypi.douban.com</span><br></pre></td></tr></table></figure></li><li>pip安装<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorboardX -i http://pypi.douban.com/</span><br><span class="line">simple --trusted-host pypi.douban.com</span><br></pre></td></tr></table></figure></li></ol><h2 id="可能遇到的错误"><a href="#可能遇到的错误" class="headerlink" title="可能遇到的错误"></a>可能遇到的错误</h2><ol><li>AttributeError: module ‘tensorflow.python.util.dispatch’ has no attribute ‘add_fallback_dispatch_list’<br><br>这个只需要升级tensorflow版本到2.9.1就可以解决</li><li>AttributeError: module ‘distutils’ has no attribute ‘version’<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   pip install setuptools==59.5.0 -i http://pypi.douban.com/</span><br><span class="line">simple --trusted-host pypi.douban.com</span><br></pre></td></tr></table></figure></li></ol><h2 id="使用心得"><a href="#使用心得" class="headerlink" title="使用心得"></a>使用心得</h2><p>还没开始用，光安装就心力憔悴了</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;官网看环境&quot;&gt;&lt;a href=&quot;#官网看环境&quot; class=&quot;headerlink&quot; title=&quot;官网看环境&quot;&gt;&lt;/a&gt;官网看环境&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/lanpa/tensorboardX&quot;&gt;tensorboar</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>夏令营冲冲冲</title>
    <link href="https://swagger-coder.github.io/2022/06/04/%E5%A4%8F%E4%BB%A4%E8%90%A5%E5%86%B2%E5%86%B2%E5%86%B2/"/>
    <id>https://swagger-coder.github.io/2022/06/04/%E5%A4%8F%E4%BB%A4%E8%90%A5%E5%86%B2%E5%86%B2%E5%86%B2/</id>
    <published>2022-06-03T21:42:00.000Z</published>
    <updated>2022-07-10T10:12:03.780Z</updated>
    
    <content type="html"><![CDATA[<h2 id="出身"><a href="#出身" class="headerlink" title="出身"></a>出身</h2><p>苏南211，软件工程专业，前五学期5/79(6%)，有一些竞赛但没有国奖，无科研，一个校级一般项目。</p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><ol><li><del>个人陈述</del></li><li>简历</li><li>申请表</li><li>教师推荐信</li><li>本人有效二代身份证件和学生证扫描件</li><li><del>在校历年学习成绩单一份</del></li><li><del>大学英语四级或六级考试成绩单复印件一份</del></li><li><del>有学术成果（包括公开发表论文、出版著作、获得专利、获得学术科技奖项、承担课题或者其他具有学术水平的工作成果），各类荣誉、表彰、奖励证书者，提供学术成果及荣誉、表彰、奖励证书等扫描件一份</del></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;出身&quot;&gt;&lt;a href=&quot;#出身&quot; class=&quot;headerlink&quot; title=&quot;出身&quot;&gt;&lt;/a&gt;出身&lt;/h2&gt;&lt;p&gt;苏南211，软件工程专业，前五学期5/79(6%)，有一些竞赛但没有国奖，无科研，一个校级一般项目。&lt;/p&gt;
&lt;h2 id=&quot;准备&quot;&gt;&lt;a </summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Python(三) 自动差异化和优化循环</title>
    <link href="https://swagger-coder.github.io/2022/06/03/Python-%E4%B8%89-%E8%87%AA%E5%8A%A8%E5%B7%AE%E5%BC%82%E5%8C%96/"/>
    <id>https://swagger-coder.github.io/2022/06/03/Python-%E4%B8%89-%E8%87%AA%E5%8A%A8%E5%B7%AE%E5%BC%82%E5%8C%96/</id>
    <published>2022-06-03T07:00:20.000Z</published>
    <updated>2022-07-10T10:12:07.627Z</updated>
    
    <content type="html"><![CDATA[<h2 id="自动差异化"><a href="#自动差异化" class="headerlink" title="自动差异化"></a>自动差异化</h2><p>在训练神经网络时，最常用的算法是反向传播。在该算法中，参数（模型权重）根据损失函数相对于给定参数（w、b）的梯度进行调整。损失函数计算神经网络产生的预期输出和实际输出之间的差异，目标是使损失函数的结果尽可能接近于零。<br>该算法通过网络向后遍历以调整权重和偏差以重新训练模型。这就是为什么它被称为反向传播。这种随着时间的推移重新训练模型以将损失减少到 0 的前后过程称为梯度下降。</p><p>在Pytorch中，这一过程将由<code>torch.autograd</code>自动完成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 通过给张量指定requires_grad=True表示需要梯度下降</span></span><br><span class="line">x = torch.ones(<span class="number">5</span>)  <span class="comment"># input tensor</span></span><br><span class="line">y = torch.zeros(<span class="number">3</span>)  <span class="comment"># expected output</span></span><br><span class="line">w = torch.randn(<span class="number">5</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = torch.matmul(x, w)+b <span class="comment"># 输出</span></span><br><span class="line">loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) <span class="comment"># 损失函数</span></span><br><span class="line"></span><br><span class="line">loss.backward() <span class="comment"># 计算梯度</span></span><br></pre></td></tr></table></figure><p>在调用<code>.backward()</code>后，<code>autograd</code>会填充一个新的图<br><img src="/2022/06/03/Python-%E4%B8%89-%E8%87%AA%E5%8A%A8%E5%B7%AE%E5%BC%82%E5%8C%96/5-autograd-1.png"></p><h3 id="禁用梯度跟踪"><a href="#禁用梯度跟踪" class="headerlink" title="禁用梯度跟踪"></a>禁用梯度跟踪</h3><p>通过<code>torch.no_grad()</code>停用梯度跟踪</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad) <span class="comment"># true</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = torch.matmul(x, w)+b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad) <span class="comment"># false</span></span><br></pre></td></tr></table></figure><h2 id="优化循环"><a href="#优化循环" class="headerlink" title="优化循环"></a>优化循环</h2><h3 id="设置超参数-hyperparameters"><a href="#设置超参数-hyperparameters" class="headerlink" title="设置超参数(hyperparameters)"></a>设置超参数(hyperparameters)</h3><ul><li>epoch</li><li>batch size</li><li>learning rate</li></ul><h3 id="添加优化循环-optimization-loop"><a href="#添加优化循环-optimization-loop" class="headerlink" title="添加优化循环(optimization loop)"></a>添加优化循环(optimization loop)</h3><p>训练将由<code>epoch</code>构成，每个<code>epoch</code>都包含<code>train loop</code>和<code>validation loop</code>。</p><h3 id="添加损失函数"><a href="#添加损失函数" class="headerlink" title="添加损失函数"></a>添加损失函数</h3><p>计算实际输出和预期之间的相异程度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><h3 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h3><p>优化器将决定如何来调整模型参数，提高准确率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br></pre></td></tr></table></figure><p>在训练循环中，优化器主要有三大作用：</p><ul><li>调用<code>optimizer.zero_grad()</code>重置模型参数的梯度矩阵（归零），梯度在默认情况下会相加。</li><li>通过调用<code>loss.backwards()</code>反向传播预测损失，Pytorch会存储每个参数的梯度。</li><li>通过调用<code>optimizer.step()</code>依据反向传播收集的梯度调整参数。</li></ul><h3 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&quot;data/model.pth&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Saved PyTorch Model State to model.pth&quot;</span>)</span><br></pre></td></tr></table></figure><p>后续的加载模型和预测没什么概念性的知识，微软的教程写的真的很清晰，有时间建议自己手敲一遍，一般实操中也就在模型上会有不同。<br>教程来源：<a href="https://docs.microsoft.com/zh-cn/learn/paths/pytorch-fundamentals/">微软-pytorch基础知识</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;自动差异化&quot;&gt;&lt;a href=&quot;#自动差异化&quot; class=&quot;headerlink&quot; title=&quot;自动差异化&quot;&gt;&lt;/a&gt;自动差异化&lt;/h2&gt;&lt;p&gt;在训练神经网络时，最常用的算法是反向传播。在该算法中，参数（模型权重）根据损失函数相对于给定参数（w、b）的梯度进行</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Pytorch(二) 生成模型层</title>
    <link href="https://swagger-coder.github.io/2022/06/02/Pytorch-%E4%BA%8C-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%B1%82/"/>
    <id>https://swagger-coder.github.io/2022/06/02/Pytorch-%E4%BA%8C-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%B1%82/</id>
    <published>2022-06-02T12:51:50.000Z</published>
    <updated>2022-07-10T10:12:15.374Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络"></a>什么是神经网络</h2><p>神经网络是由层连接的神经元的集合。每个神经元都是一个小型计算单元，它执行简单的计算以共同解决一个问题。它们按层组织。有 3 种类型的层：输入层、隐藏层和外层。除输入层外，每一层都包含许多神经元。神经网络模仿人脑处理信息的方式<br><img src="/2022/06/02/Pytorch-%E4%BA%8C-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%B1%82/4-model-1.png"></p><h2 id="神经网络的组件"><a href="#神经网络的组件" class="headerlink" title="神经网络的组件"></a>神经网络的组件</h2><ul><li><p>激活函数<br><br>决定一个神经元是否应该被激活。神经网络中发生的计算包括应用激活函数。如果一个神经元被激活，那么这意味着输入很重要。有很多不同种类的激活函数。使用哪个激活函数的选择取决于您想要的输出。激活函数的另一个重要作用是为模型添加非线性。</p><ul><li><code>Binary</code>：输出为正=1，输出为负=0</li><li><code>Sigmoid</code>：预测可能性，值在(0,1)</li><li><code>Tanh</code>：Tanh 用于预测输出节点是否介于 1 和 -1 之间。用于分类用例。</li><li><code>ReLU</code>：ReLU 用于在函数结果为负时将输出节点设置为 0，如果结果为正值则保留结果值。</li></ul></li><li><p>权重<br><br>权重会影响我们网络的输出与预期输出值的接近程度。当输入进入神经元时，它会乘以权重值，然后观察结果输出，或者将其传递到神经网络中的下一层。一层中所有神经元的权重被组织成一个张量。</p></li><li><p>偏差<br><br>偏差弥补了激活函数的输出与其预期输出之间的差异。低偏差表明网络对输出的形式做出了更多的假设，而高偏差值对输出的形式做出了较少的假设。</p></li></ul><h2 id="构建神经网络"><a href="#构建神经网络" class="headerlink" title="构建神经网络"></a>构建神经网络</h2><p>神经网络由<code>layers/modules</code>组成，在pytorch中，所有的<code>module</code>需要继承<code>nn.Module</code>，一个神经网络本身也是组合了很多层的一个<code>module</code>。</p><h3 id="定义类"><a href="#定义类" class="headerlink" title="定义类"></a>定义类</h3><p>我们通过子类化 <code>nn.Module</code> 来定义我们的神经网络，并在 <code>__init__</code> 中初始化神经网络层。每个 <code>nn.Module </code>子类都在 <code>forward</code> 方法中实现对输入数据的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNetwork, self).__init__()</span><br><span class="line">        <span class="comment"># 平面化(bs, a, b) -&gt; (bs, a*b)</span></span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        <span class="comment"># nn.Sequential是有序的模块容器</span></span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),  <span class="comment"># 线性层</span></span><br><span class="line">            nn.ReLU(),  <span class="comment"># 激活函数</span></span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"><span class="comment"># 应用模型</span></span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device=device)    <span class="comment"># input Size([1,28,28])</span></span><br><span class="line">logits = model(X)  <span class="comment"># output</span></span><br><span class="line">pred_probab = nn.Softmax(dim=<span class="number">1</span>)(logits) <span class="comment"># 激活函数，计算输出概率，仅用于输出层。dim 参数指示结果值总和必须为 1 的维度。</span></span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>) <span class="comment"># 输出概率最高的节点</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h2><h3 id="nn-linear"><a href="#nn-linear" class="headerlink" title="nn.linear"></a>nn.linear</h3><p>线性层会随机初始化<code>weights</code>和<code>bias</code>，并且在内部将值存储在张量中。<br><br>神经网络中的许多层都是参数化的，即具有在训练期间优化的相关权重和偏差。子类化 <code>nn.Module</code> 会自动跟踪模型对象中定义的所有字段，并使用模型的 <code>parameters()</code> 或 <code>named_pa​​rameters()</code> 方法使所有参数都可以访问。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model structure: &quot;</span>, model, <span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Layer: <span class="subst">&#123;name&#125;</span> | Size: <span class="subst">&#123;param.size()&#125;</span> | Values : <span class="subst">&#123;param[:<span class="number">2</span>]&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><p>教程来源：<a href="https://docs.microsoft.com/zh-cn/learn/paths/pytorch-fundamentals/">微软-pytorch基础知识</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;什么是神经网络&quot;&gt;&lt;a href=&quot;#什么是神经网络&quot; class=&quot;headerlink&quot; title=&quot;什么是神经网络&quot;&gt;&lt;/a&gt;什么是神经网络&lt;/h2&gt;&lt;p&gt;神经网络是由层连接的神经元的集合。每个神经元都是一个小型计算单元，它执行简单的计算以共同解决一个问题</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Pytorch(一) 什么是张量？</title>
    <link href="https://swagger-coder.github.io/2022/06/02/Pytorch-%E4%B8%80-%E4%BB%80%E4%B9%88%E6%98%AF%E5%BC%A0%E9%87%8F%EF%BC%9F/"/>
    <id>https://swagger-coder.github.io/2022/06/02/Pytorch-%E4%B8%80-%E4%BB%80%E4%B9%88%E6%98%AF%E5%BC%A0%E9%87%8F%EF%BC%9F/</id>
    <published>2022-06-02T08:22:11.000Z</published>
    <updated>2022-07-10T10:12:11.411Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><ul><li>python</li><li>线性代数</li><li>numpy</li></ul><h2 id="初始化张量"><a href="#初始化张量" class="headerlink" title="初始化张量"></a>初始化张量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.直接从数据</span></span><br><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>],[<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br><span class="line"><span class="comment"># 2.从numpy数组</span></span><br><span class="line">np_array = np.array(data)</span><br><span class="line">x_np = torch.from_numpy(np_array)</span><br><span class="line"><span class="comment"># 3.从另一个tensor</span></span><br><span class="line">x_ones = torch.ones_like(x_data) <span class="comment"># 保留原张量大小和数据类型</span></span><br><span class="line">x_rand = torch.rand_like(x_data, dtype=torch.long) <span class="comment"># 改变数据类型</span></span><br><span class="line"><span class="comment"># 4.具有随机或恒定值</span></span><br><span class="line">shape = (<span class="number">2</span>,<span class="number">3</span>,) <span class="comment"># tensor的维度</span></span><br><span class="line">rand_tensor = torch.rand(shape) <span class="comment"># 0-1之间</span></span><br><span class="line">ones_tensor = torch.ones(shape) <span class="comment"># 全1</span></span><br><span class="line">zeros_tensor = torch.zeros(shape) <span class="comment"># 全0</span></span><br><span class="line">rand_tensor = torch.rand(<span class="number">3</span>, <span class="number">4</span>) <span class="comment"># size[3,4] datatype: torch.float32</span></span><br></pre></td></tr></table></figure><h2 id="张量的属性"><a href="#张量的属性" class="headerlink" title="张量的属性"></a>张量的属性</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">tensor.shape <span class="comment"># torch.Size([3, 4])</span></span><br><span class="line">tensor.dtype <span class="comment"># torch.float32</span></span><br></pre></td></tr></table></figure><h2 id="张量的操作"><a href="#张量的操作" class="headerlink" title="张量的操作"></a>张量的操作</h2><h3 id="类似numpy的索引和切片"><a href="#类似numpy的索引和切片" class="headerlink" title="类似numpy的索引和切片"></a>类似numpy的索引和切片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">tensor[<span class="number">0</span>] <span class="comment"># First row</span></span><br><span class="line">tensor[:, <span class="number">0</span>] <span class="comment"># First column</span></span><br><span class="line">tensor[..., -<span class="number">1</span>] <span class="comment"># Last column</span></span><br><span class="line">tensor[:,<span class="number">1</span>] = <span class="number">0</span> <span class="comment"># 第1列设为0</span></span><br></pre></td></tr></table></figure><h3 id="tensor拼接"><a href="#tensor拼接" class="headerlink" title="tensor拼接"></a>tensor拼接</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="number">1</span>) <span class="comment"># dim=1按列拼接，行数不变；dim=0按行拼接，列数不变</span></span><br></pre></td></tr></table></figure><h3 id="算数运算"><a href="#算数运算" class="headerlink" title="算数运算"></a>算数运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value 矩阵乘法</span></span><br><span class="line">y1 = tensor @ tensor.T</span><br><span class="line">y2 = tensor.matmul(tensor.T)</span><br><span class="line"></span><br><span class="line">y3 = torch.rand_like(tensor)</span><br><span class="line">torch.matmul(tensor, tensor.T, out=y3)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># This computes the element-wise product. z1, z2, z3 will have the same value 对应位置元素相乘</span></span><br><span class="line">z1 = tensor * tensor</span><br><span class="line">z2 = tensor.mul(tensor)</span><br><span class="line"></span><br><span class="line">z3 = torch.rand_like(tensor)</span><br><span class="line">torch.mul(tensor, tensor, out=z3)</span><br></pre></td></tr></table></figure><h3 id="原地操作"><a href="#原地操作" class="headerlink" title="原地操作"></a>原地操作</h3><p>不鼓励使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 所有元素加5</span></span><br><span class="line">tensor.add_(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>教程来源：<a href="https://docs.microsoft.com/zh-cn/learn/paths/pytorch-fundamentals/">微软-pytorch基础知识</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前置知识&quot;&gt;&lt;a href=&quot;#前置知识&quot; class=&quot;headerlink&quot; title=&quot;前置知识&quot;&gt;&lt;/a&gt;前置知识&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;python&lt;/li&gt;
&lt;li&gt;线性代数&lt;/li&gt;
&lt;li&gt;numpy&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;初</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>transformer--Preprocess</title>
    <link href="https://swagger-coder.github.io/2022/05/27/transformer-Preprocess/"/>
    <id>https://swagger-coder.github.io/2022/05/27/transformer-Preprocess/</id>
    <published>2022-05-27T06:13:36.000Z</published>
    <updated>2022-06-02T08:22:38.414Z</updated>
    
    <content type="html"><![CDATA[<h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>在把数据放入模型之前，先要进行预处理，将文本、图像、音频转换为模型能够处理的张量。</p><h2 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h2><p>在NLP中，最常用的预处理工具是<code>tokenizer</code>(分词器)，分词器就是将句子分割成词，然后将词按照词典对应到唯一的数字。首先搞清楚<code>tokenizer</code>的输入和输出：<br></p><ul><li><code>input</code>：单个文本，多个文本或文本列表。需要注意文本列表返回的张量会改变维度</li><li><code>output</code>：包含<code>input_ids</code>、<code>attention_mask</code>、<code>token_type_ids</code>的字典<ul><li><code>input_ids</code>：句子中每个词对应的索引</li><li><code>attention_mask</code>：指示信息的有用性（填充的为0）</li><li><code>token_type_ids</code>：多个句子时指示索引属于哪个句子</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;预处理&quot;&gt;&lt;a href=&quot;#预处理&quot; class=&quot;headerlink&quot; title=&quot;预处理&quot;&gt;&lt;/a&gt;预处理&lt;/h2&gt;&lt;p&gt;在把数据放入模型之前，先要进行预处理，将文本、图像、音频转换为模型能够处理的张量。&lt;/p&gt;
&lt;h2 id=&quot;NLP&quot;&gt;&lt;a href</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>简历培训</title>
    <link href="https://swagger-coder.github.io/2022/05/25/%E7%AE%80%E5%8E%86%E5%9F%B9%E8%AE%AD/"/>
    <id>https://swagger-coder.github.io/2022/05/25/%E7%AE%80%E5%8E%86%E5%9F%B9%E8%AE%AD/</id>
    <published>2022-05-25T11:05:29.000Z</published>
    <updated>2022-07-10T10:12:27.898Z</updated>
    
    <content type="html"><![CDATA[<h2 id="好简历背后的逻辑"><a href="#好简历背后的逻辑" class="headerlink" title="好简历背后的逻辑"></a>好简历背后的逻辑</h2><p>专属、定位、精准</p><h4 id="亮点突出"><a href="#亮点突出" class="headerlink" title="亮点突出"></a>亮点突出</h4><p>岗位导向、结果导向、权重导向<br><br>就个人而言，先要选择好岗位（研发or管理等）<br><img src="/2022/05/25/%E7%AE%80%E5%8E%86%E5%9F%B9%E8%AE%AD/1.png"></p><h4 id="细节支撑"><a href="#细节支撑" class="headerlink" title="细节支撑"></a>细节支撑</h4><ul><li>时间</li><li>地点</li><li>人物</li><li>事件</li><li>数据<br>要写实打实的，可以经得住拷问<br><br><img src="/2022/05/25/%E7%AE%80%E5%8E%86%E5%9F%B9%E8%AE%AD/2.png"></li></ul><h4 id="格式整洁"><a href="#格式整洁" class="headerlink" title="格式整洁"></a>格式整洁</h4><p>精简、详略得当</p><h4 id="真实可信"><a href="#真实可信" class="headerlink" title="真实可信"></a>真实可信</h4><p>可包装不能浮夸</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;好简历背后的逻辑&quot;&gt;&lt;a href=&quot;#好简历背后的逻辑&quot; class=&quot;headerlink&quot; title=&quot;好简历背后的逻辑&quot;&gt;&lt;/a&gt;好简历背后的逻辑&lt;/h2&gt;&lt;p&gt;专属、定位、精准&lt;/p&gt;
&lt;h4 id=&quot;亮点突出&quot;&gt;&lt;a href=&quot;#亮点突出&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>STQA 移动应用测试学习笔记</title>
    <link href="https://swagger-coder.github.io/2022/05/25/STQA-%E7%A7%BB%E5%8A%A8%E5%BA%94%E7%94%A8%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://swagger-coder.github.io/2022/05/25/STQA-%E7%A7%BB%E5%8A%A8%E5%BA%94%E7%94%A8%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
    <published>2022-05-25T01:35:49.000Z</published>
    <updated>2022-07-10T10:12:24.069Z</updated>
    
    <content type="html"><![CDATA[<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>1.含mooctest插件的eclipse<br><br>2.jdk<br><br>3.Android SDK<br><br>4.Node.js<br><br>5.Appium<br><br>6.安卓模拟器（<a href="https://mumu.163.com/index.html">mumu</a>、雷电模拟器等）<br><br>7.判断所有环境配置成功<br></p><h2 id="脚本编写"><a href="#脚本编写" class="headerlink" title="脚本编写"></a>脚本编写</h2><h4 id="1-APP控件定位工具"><a href="#1-APP控件定位工具" class="headerlink" title="1. APP控件定位工具"></a>1. APP控件定位工具</h4><ul><li>打开安卓模拟器，<code>adb devices</code>测试手机是否与电脑连接（mumu<code>adb connect 127.0.0.1:7555</code>）</li><li>打开定位工具，我的安装目录在<code>C:\Program Files\android-sdk-windows\tools</code>，找到<code>uiautomatorviewer.bat</code>，即可截取手机屏幕，获取控件信息</li></ul><h4 id="2-控件定位方式"><a href="#2-控件定位方式" class="headerlink" title="2. 控件定位方式"></a>2. 控件定位方式</h4><ul><li>ById：有唯一resource-id<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">driver.findElementById(&quot;com.alensw.PicFolder:id/search&quot;).click();</span><br><span class="line">driver.findElementById(&quot;com.alensw.PicFolder:id/search&quot;).sendKeys(&quot;hello&quot;);</span><br></pre></td></tr></table></figure></li><li>ByXPath：通过class+text/index/content-desc（推荐）获取<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">driver.findElementByXPath(&quot;//android.widget.TextView[@text=&#x27;相册&#x27;]&quot;).click();</span><br><span class="line">driver.findElementByXPath(&quot;//android.widget.EditText[@index=&#x27;0&#x27;]&quot;).click();</span><br><span class="line">driver.findElementByXPath(&quot;//android.widget.ImageButton[@content-desc=&#x27;向上导航&#x27;]&quot;).click();</span><br></pre></td></tr></table></figure></li></ul><h4 id="3-流程"><a href="#3-流程" class="headerlink" title="3. 流程"></a>3. 流程</h4><ul><li>打开模拟器，安装好apk</li><li>adb连接模拟器和电脑</li><li>打开Appium</li><li>打开APP控件定位工具</li><li>编写代码</li></ul><p>附上<a href="https://www.aliyundrive.com/s/PLwbADzoqc9">测试环境配置和脚本编写ppt</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;环境准备&lt;/h2&gt;&lt;p&gt;1.含mooctest插件的eclipse&lt;br&gt;&lt;br&gt;2.jdk&lt;br&gt;&lt;br&gt;3.Android SDK&lt;br&gt;&lt;br</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>微信大数据挑战赛学习笔记</title>
    <link href="https://swagger-coder.github.io/2022/05/23/%E5%BE%AE%E4%BF%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8C%91%E6%88%98%E8%B5%9B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>https://swagger-coder.github.io/2022/05/23/%E5%BE%AE%E4%BF%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8C%91%E6%88%98%E8%B5%9B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
    <published>2022-05-23T14:25:28.000Z</published>
    <updated>2022-07-10T10:12:31.734Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一些常识"><a href="#一些常识" class="headerlink" title="一些常识"></a>一些常识</h2><ul><li>mask<br><br>是相对于PAD而产生的技术，只有0和1两个值，0表示的值对应PAD矩阵中对应位置的值无意义。<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><img src="/2022/05/23/%E5%BE%AE%E4%BF%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8C%91%E6%88%98%E8%B5%9B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1.png"><br>想要有好的结果，首先要对数据进行预处理，可以看到，有用信息就是String类型：title，asr，ocr和float类型：frames_feature</li></ul><p>tokenizer就是分词器</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul><li>什么是预训练模型（hfl/chinese-macbert-base）</li><li>什么是对抗</li><li>ema？指数滑动平均</li><li>怎么换模型，chinese-roberta-wwm-ext</li><li>fgm<br><br><a href="https://github.com/rsanshierli/Bert-Classification-EMA-AD/blob/master/FGM.py">https://github.com/rsanshierli/Bert-Classification-EMA-AD/blob/master/FGM.py</a></li></ul><h2 id="tricks"><a href="#tricks" class="headerlink" title="tricks"></a>tricks</h2><ul><li>对抗</li><li>ema</li><li>fgm</li><li>学习率更新</li></ul><h2 id="一些小坑"><a href="#一些小坑" class="headerlink" title="一些小坑"></a>一些小坑</h2><ul><li>线上线下分数差距大<br><br>原因是验证集划分不均，和训练集相似度过高</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一些常识&quot;&gt;&lt;a href=&quot;#一些常识&quot; class=&quot;headerlink&quot; title=&quot;一些常识&quot;&gt;&lt;/a&gt;一些常识&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;mask&lt;br&gt;&lt;br&gt;是相对于PAD而产生的技术，只有0和1两个值，0表示的值对应PAD矩阵中对应位置的值无意</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>tornado 学习笔记（一）</title>
    <link href="https://swagger-coder.github.io/2022/05/11/tornado-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://swagger-coder.github.io/2022/05/11/tornado-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2022-05-11T14:13:42.000Z</published>
    <updated>2022-07-10T10:12:36.342Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tornado是如何做到高并发的"><a href="#tornado是如何做到高并发的" class="headerlink" title="tornado是如何做到高并发的"></a>tornado是如何做到高并发的</h2><ul><li>异步非阻塞io</li><li>基于epoll的<strong>事件循环</strong><br><br>win不支持</li><li><strong>协程</strong>提高了代码的可读性<blockquote><p>事件循环和协程究竟是什么呢？</p></blockquote></li></ul><h2 id><a href="#" class="headerlink" title></a></h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;tornado是如何做到高并发的&quot;&gt;&lt;a href=&quot;#tornado是如何做到高并发的&quot; class=&quot;headerlink&quot; title=&quot;tornado是如何做到高并发的&quot;&gt;&lt;/a&gt;tornado是如何做到高并发的&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;异步非阻塞io&lt;</summary>
      
    
    
    
    
    <category term="tornado" scheme="https://swagger-coder.github.io/tags/tornado/"/>
    
  </entry>
  
  <entry>
    <title>Python virtualenv的安装配置</title>
    <link href="https://swagger-coder.github.io/2022/05/11/Python-virtualenv%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"/>
    <id>https://swagger-coder.github.io/2022/05/11/Python-virtualenv%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/</id>
    <published>2022-05-11T10:51:07.000Z</published>
    <updated>2022-07-10T10:12:44.432Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>通过pip安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install virtualenv</span><br></pre></td></tr></table></figure><p>并安装virtualenvwrapper，方便管理虚拟环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#win系统安装</span></span><br><span class="line">pip install virtualenvwrapper-win</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建虚拟环境 默认地址在 C:\Users\username\Envs</span></span><br><span class="line">mkvirtualenv envname</span><br><span class="line"></span><br><span class="line"><span class="comment">#退出虚拟环境</span></span><br><span class="line">deactivate</span><br><span class="line"></span><br><span class="line"><span class="comment">#进入指定虚拟环境</span></span><br><span class="line">workon envname</span><br><span class="line"></span><br><span class="line"><span class="comment">#指定虚拟环境python版本</span></span><br><span class="line">mkvirtualenv -p [pypath] envname</span><br></pre></td></tr></table></figure><p>虚拟环境配置好之后记得在开发的时候选择虚拟环境中的python解释器</p><h2 id="采用复制方法迁移python虚拟环境"><a href="#采用复制方法迁移python虚拟环境" class="headerlink" title="采用复制方法迁移python虚拟环境"></a>采用复制方法迁移python虚拟环境</h2><p>迁移后需要在相应目录修改路径。</p><ol><li>整体复制虚拟环境的目录</li><li>修改以下文件的内容：</li></ol><ul><li>activate</li><li>activate.bat</li><li>Activate.ps1</li></ul><p>将里面的路径改成当前的路径，用查找文件内容的方式改，免得漏了。<br><br>activate是linux下面进入虚拟环境的执行文件。在windows下，主要起作用的是 activate.bat和Activate.ps1。<br><br>1）activate.bat 下需要修改的位置：<br></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">11行：set &quot;VIRTUAL_ENV=E:\Py_ENV\venv2&quot;</span><br><span class="line">26行：set &quot;PROMPT=(venv2) %PROMPT%&quot;</span><br></pre></td></tr></table></figure><p>2）Activate.ps1下需要修改的位置：<br></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">30行：$env:VIRTUAL_ENV=&quot;E:\Py_ENV\venv2&quot;</span><br><span class="line">38行：Write-Host -NoNewline -ForegroundColor Green &#x27;(venv2) &#x27;</span><br></pre></td></tr></table></figure><ol start="3"><li>重新安装PIP。<br><br>为什么要做这个步骤？因为复制虚拟环境目录后，pip安装的路径还会之前的，可以用记事本查看pip.exe文件的最后。从而用pip install xxx安装的第三方模块的路径都会搞错。<br><br>因此，可手动删除 【..\Lib\site-packages】目录下的【pip-19.1.1.dist-info】文件夹。然后进入虚拟环境，执行pip安装命令后即可<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install -U pip</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h2&gt;&lt;p&gt;通过pip安装&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutte</summary>
      
    
    
    
    
    <category term="python" scheme="https://swagger-coder.github.io/tags/python/"/>
    
  </entry>
  
</feed>
